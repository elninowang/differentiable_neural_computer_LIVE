{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Differentiable Neural Computer 可微分神经计算机\n",
    "\n",
    "## The Problem - how do we create more general purpose learning machines?\n",
    "\n",
    "## 问题 - 我们如何创建更多的通用学习机器？\n",
    "\n",
    "Neural networks excel at pattern recognition and quick, reactive decision-making, but we are only just \n",
    "beginning to build neural networks that can think slowly. that is, deliberate or reason using knowledge.\n",
    "For example, how could a neural network store memories for facts like the connections in a transport network \n",
    "and then logically reason about its pieces of knowledge to answer questions?\n",
    "\n",
    "神经网络在模式识别和快速，反应性决策方面表现优异，但我们只是这样开始构建可以慢慢思考的神经网络。 那就是使用知识的故意或理性。例如，神经网络如何存储事实，如传输网络中的连接然后在逻辑上推理其知识点来回答问题？\n",
    "\n",
    "![alt text](https://storage.googleapis.com/deepmind-live-cms/images/dnc_figure1.width-1500_Zfxk87k.png \"Logo Title Text 1\")\n",
    "\n",
    "this consists of a neural network that can read from and write to an external memory matrix,\n",
    "analogous to the random-access memory in a conventional computer.\n",
    "\n",
    "这包括可以从外部存储器矩阵读取和写入的神经网络，类似于常规计算机中的随机存取存储器。\n",
    "\n",
    "Like a conventional computer, it can use its memory to represent and manipulate complex data structures, \n",
    "but, like a neural network, it can learn to do so from data.\n",
    "\n",
    "像传统的计算机一样，它可以使用其内存来表示和操纵复杂的数据结构，更是，像神经网络一样，它可以从数据中学习。\n",
    "\n",
    "DNCs have the capacity to solve complex, structured tasks that are \n",
    "inaccessible to neural networks without external read–write memory.\n",
    "\n",
    "DNC有能力解决复杂，结构化的任务，没有外部读写存储器的神经网络无法访问。\n",
    "\n",
    "![alt text](https://storage.googleapis.com/deepmind-live-cms/images/dnc_figure2.width-1500_be2TeKT.png \"Logo Title Text 1\")\n",
    "\n",
    "[![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/B9U8sI7TcMY/0.jpg)](http://www.youtube.com/watch?v=B9U8sI7TcMYE)\n",
    "\n",
    "Modern computers separate computation and memory. Computation is performed by a processor, \n",
    "which can use an addressable memory to bring operands in and out of play. \n",
    "\n",
    "现代电脑分开计算和记忆。 计算由处理器执行，其可以使用可寻址存储器来使操作数进出。\n",
    "\n",
    "In contrast to computers, the computational and memory resources of artificial neural networks \n",
    "are mixed together in the network weights and neuron activity. This is a major liability: \n",
    "as the memory demands of a task increase, these networks cannot allocate new storage \n",
    "dynam-ically, nor easily learn algorithms that act independently of the values realized \n",
    "by the task variables.\n",
    "\n",
    "与计算机相比，人工神经网络的计算和存储资源在网络权重和神经元活动中混合在一起。 这是一个主要的责任：随着任务的内存需求增加，这些网络不能分配新的存储空间动态地，也不容易学习独立于任务变量实现的值的算法\n",
    "    \n",
    "The whole system is differentiable, and can therefore be trained end-to-end with gradient descent, allowing the network to learn how to operate and organize the memory in a goal-directed manner.\n",
    "\n",
    "整个系统是可以区分的，因此可以通过梯度下降进行端到端的训练，从而允许网络以目标方式学习如何操作和组织记忆。\n",
    "\n",
    "If the memory can be thought of as the DNC’s RAM, then the network, referred to as the ‘controller’, is a differentiable CPU whose operations are learned with gradient descent.\n",
    "\n",
    "如果内存可以被认为是DNC的RAM，那么称为“控制器”的网络是一个可微分的CPU，其操作是通过梯度下降来学习的。\n",
    "\n",
    "How is it different from its predecessor, the Neural Turing Machine?\n",
    "\n",
    "与之前的神经图灵机有何不同？\n",
    "\n",
    "basically, more memory access methods than NTM 基本上比NTM更多的内存访问方法\n",
    "\n",
    " DNC extends the NTM addressing the following limitations:  DNC扩展了NTM，解决了以下限制：\n",
    " \n",
    "(1) Ensuring that blocks of allocated memory do not overlap and interfere. 确保分配的内存块不重叠和干扰。\n",
    "\n",
    "(2) Freeing memory that have already been written to. 释放已写入的内存。\n",
    "\n",
    "(3) Handling of non-contiguous memory through temporal links. 通过时间链接处理不连续的内存。\n",
    "\n",
    "the system required hand-crafted input to accomplish its learning and inference. This is not an NLP system where unstructured text is applied at input. \n",
    "\n",
    "该系统需要手工输入来完成其学习和推断。 这不是在输入中应用非结构化文本的NLP系统。\n",
    "\n",
    "3 forms of attention for heads  头脑的3种注意形式\n",
    "\n",
    "- content lookup\n",
    "- records transitions between consecutively written locations in an N × N temporal link matrix L.\n",
    "This gives a DNC the native ability to recover sequences in the order in which it wrote them, even when consecutive writes did not occur in adjacent time-step\n",
    "- The third form of attention allocates memory for writing. \n",
    "\n",
    "- 内容查询\n",
    "- 在N×N个时间链接矩阵L中记录连续写入位置之间的转换\n",
    "这给DNC提供了按照写入顺序来恢复序列的本机能力，即使连续的写入没有在相邻的时间步长中发生\n",
    "- 第三种注意形式分配内存写入。\n",
    "\n",
    "Content lookup enables the formation of associative data structures; \n",
    "temporal links enable sequential retrieval of input sequences;\n",
    "and allocation provides the write head with unused locations. \n",
    "\n",
    "内容查找能够形成关联数据结构;\n",
    "时间链接使序列检索输入序列;\n",
    "并且分配向写头提供未使用的位置。\n",
    "\n",
    "DNC memory modification is fast and can be one-shot, resembling the associative long-term potentiation of hippocampal CA3 and CA1 synapses\n",
    "\n",
    "DNC记忆修饰速度快，可以一次性，类似于海马体CA3和CA1突触的联合长期增强作用\n",
    "\n",
    "Human ‘free recall’ experiments demonstrate the increased probability of item recall in the same order as first pre-sented (temporal links)\n",
    " \n",
    "人类“自由回忆”实验表明，项目召回的概率按照与第一次预先（时间链接）相同的顺序进行，\n",
    "    \n",
    "DeepMind hopes that DNCs provide both a new tool for computer science and a new metaphor for cognitive science and neuroscience: here is a learning machine that, without prior programming, can organise information into connected facts and use those facts to solve problems.\n",
    "    \n",
    "DeepMind希望DNC提供计算机科学的新工具和认知科学和神经科学的新隐喻：这是一种学习机器，无需事先编程就可以将信息组织在连接的事实中，并利用这些事实来解决问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DNC:\n",
    "    def __init__(self, input_size, output_size, seq_len, num_words=256, word_size=64, num_heads=4):\n",
    "        #define data\n",
    "        #input data - [[1 0] [0 1] [0 0] [0 0]]\n",
    "        self.input_size = input_size #X\n",
    "        #output data [[0 0] [0 0] [1 0] [0 1]]\n",
    "        self.output_size = output_size #Y\n",
    "        \n",
    "        #define read + write vector size\n",
    "        #10\n",
    "        self.num_words = num_words #N\n",
    "        #4 characters\n",
    "        self.word_size = word_size #W\n",
    "        \n",
    "        #define number of read+write heads\n",
    "        #we could have multiple, but just 1 for simplicity\n",
    "        self.num_heads = num_heads #R\n",
    "\n",
    "        #size of output vector from controller that defines interactions with memory matrix\n",
    "        self.interface_size = num_heads*word_size + 3*word_size + 5*num_heads + 3\n",
    "\n",
    "        #the actual size of the neural network input after flatenning and\n",
    "        # concatenating the input vector with the previously read vctors from memory\n",
    "        self.nn_input_size = num_heads * word_size + input_size\n",
    "        \n",
    "        #size of output\n",
    "        self.nn_output_size = output_size + self.interface_size\n",
    "        \n",
    "        #gaussian normal distribution for both outputs\n",
    "        self.nn_out = tf.truncated_normal([1, self.output_size], stddev=0.1)\n",
    "        self.interface_vec = tf.truncated_normal([1, self.interface_size], stddev=0.1)\n",
    "\n",
    "        #Create memory matrix\n",
    "        self.mem_mat = tf.zeros([num_words, word_size]) #N*W\n",
    "        \n",
    "        #other variables\n",
    "        #The usage vector records which locations have been used so far, \n",
    "        self.usage_vec = tf.fill([num_words, 1], 1e-6) #N*1\n",
    "        #a temporal link matrix records the order in which locations were written;\n",
    "        self.link_mat = tf.zeros([num_words,num_words]) #N*N\n",
    "        #represents degrees to which last location was written to\n",
    "        self.precedence_weight = tf.zeros([num_words, 1]) #N*1\n",
    "\n",
    "        #Read and write head variables\n",
    "        self.read_weights = tf.fill([num_words, num_heads], 1e-6) #N*R\n",
    "        self.write_weights = tf.fill([num_words, 1], 1e-6) #N*1\n",
    "        self.read_vecs = tf.fill([num_heads, word_size], 1e-6) #R*W\n",
    "\n",
    "        ###NETWORK VARIABLES\n",
    "        #gateways into the computation graph for input output pairs\n",
    "        self.i_data = tf.placeholder(tf.float32, [seq_len*2, self.input_size], name='input_node')\n",
    "        self.o_data = tf.placeholder(tf.float32, [seq_len*2, self.output_size], name='output_node')\n",
    "        \n",
    "        #2 layer feedforwarded network\n",
    "        self.W1 = tf.Variable(tf.truncated_normal([self.nn_input_size, 32], stddev=0.1), name='layer1_weights', dtype=tf.float32)\n",
    "        self.b1 = tf.Variable(tf.zeros([32]), name='layer1_bias', dtype=tf.float32)\n",
    "        self.W2 = tf.Variable(tf.truncated_normal([32, self.nn_output_size], stddev=0.1), name='layer2_weights', dtype=tf.float32)\n",
    "        self.b2 = tf.Variable(tf.zeros([self.nn_output_size]), name='layer2_bias', dtype=tf.float32)\n",
    "\n",
    "        ###DNC OUTPUT WEIGHTS\n",
    "        self.nn_out_weights = tf.Variable(tf.truncated_normal([self.nn_output_size, self.output_size], stddev=0.1), name='net_output_weights')\n",
    "        self.interface_weights = tf.Variable(tf.truncated_normal([self.nn_output_size, self.interface_size], stddev=0.1), name='interface_weights')\n",
    "        \n",
    "        self.read_vecs_out_weight = tf.Variable(tf.truncated_normal([self.num_heads*self.word_size, self.output_size], stddev=0.1), name='read_vector_weights')\n",
    "\n",
    "    #3 attention mechanisms for read/writes to memory \n",
    "    \n",
    "    #1\n",
    "    #a key vector emitted by the controller is compared to the \n",
    "    #content of each location in memory according to a similarity measure \n",
    "    #The similarity scores determine a weighting that can be used by the read heads \n",
    "    #for associative recall1 or by the write head to modify an existing vector in memory.\n",
    "    def content_lookup(self, key, str):\n",
    "        #The l2 norm of a vector is the square root of the sum of the \n",
    "        #absolute values squared\n",
    "        norm_mem = tf.nn.l2_normalize(self.mem_mat, 1) #N*W\n",
    "        norm_key = tf.nn.l2_normalize(key, 0) #1*W for write or R*W for read\n",
    "        #get similarity measure between both vectors, transpose before multiplicaiton\n",
    "        ##(N*W,W*1)->N*1 for write\n",
    "        #(N*W,W*R)->N*R for read\n",
    "        sim = tf.matmul(norm_mem, norm_key, transpose_b=True) \n",
    "        #str is 1*1 or 1*R\n",
    "        #returns similarity measure\n",
    "        return tf.nn.softmax(sim*str, 0) #N*1 or N*R\n",
    "\n",
    "    #2\n",
    "    #retreives the writing allocation weighting based on the usage free list\n",
    "    #The ‘usage’ of each location is represented as a number between 0 and 1, \n",
    "    #and a weighting that picks out unused locations is delivered to the write head. \n",
    "    \n",
    "    # independent of the size and contents of the memory, meaning that \n",
    "    #DNCs can be trained to solve a task using one size of memory and later \n",
    "    #upgraded to a larger memory without retraining\n",
    "    def allocation_weighting(self):\n",
    "        #sorted usage - the usage vector sorted ascndingly\n",
    "        #the original indices of the sorted usage vector\n",
    "        sorted_usage_vec, free_list = tf.nn.top_k(-1 * self.usage_vec, k=self.num_words)\n",
    "        sorted_usage_vec *= -1\n",
    "        cumprod = tf.cumprod(sorted_usage_vec, axis=0, exclusive=True)\n",
    "        unorder = (1-sorted_usage_vec)*cumprod\n",
    "\n",
    "        alloc_weights = tf.zeros([self.num_words])\n",
    "        I = tf.constant(np.identity(self.num_words, dtype=np.float32))\n",
    "        \n",
    "        #for each usage vec\n",
    "        for pos, idx in enumerate(tf.unstack(free_list[0])):\n",
    "            #flatten\n",
    "            m = tf.squeeze(tf.slice(I, [idx, 0], [1, -1]))\n",
    "            #add to weight matrix\n",
    "            alloc_weights += m*unorder[0, pos]\n",
    "        #the allocation weighting for each row in memory\n",
    "        return tf.reshape(alloc_weights, [self.num_words, 1])\n",
    "\n",
    "    #at every time step the controller receives input vector from dataset and emits output vector. \n",
    "    #it also recieves a set of read vectors from the memory matrix at the previous time step via \n",
    "    #the read heads. then it emits an interface vector that defines its interactions with the memory\n",
    "    #at the current time step\n",
    "    def step_m(self, x):\n",
    "        \n",
    "        #reshape input\n",
    "        input = tf.concat([x, tf.reshape(self.read_vecs, [1, self.num_heads*self.word_size])],1)\n",
    "        \n",
    "        #forward propagation\n",
    "        l1_out = tf.matmul(input, self.W1) + self.b1\n",
    "        l1_act = tf.nn.tanh(l1_out)\n",
    "        l2_out = tf.matmul(l1_act, self.W2) + self.b2\n",
    "        l2_act = tf.nn.tanh(l2_out)\n",
    "        \n",
    "        #output vector\n",
    "        self.nn_out = tf.matmul(l2_act, self.nn_out_weights) #(1*eta+Y, eta+Y*Y)->(1*Y)\n",
    "        #interaction vector - how to interact with memory\n",
    "        self.interface_vec = tf.matmul(l2_act, self.interface_weights) #(1*eta+Y, eta+Y*eta)->(1*eta)\n",
    "        \n",
    "        \n",
    "        partition = tf.constant([[0]*(self.num_heads*self.word_size) + [1]*(self.num_heads) + [2]*(self.word_size) + [3] + \\\n",
    "                    [4]*(self.word_size) + [5]*(self.word_size) + \\\n",
    "                    [6]*(self.num_heads) + [7] + [8] + [9]*(self.num_heads*3)], dtype=tf.int32)\n",
    "\n",
    "        #convert interface vector into a set of read write vectors\n",
    "        #using tf.dynamic_partitions(Partitions interface_vec into 10 tensors using indices from partition)\n",
    "        (read_keys, read_str, write_key, write_str,\n",
    "         erase_vec, write_vec, free_gates, alloc_gate, write_gate, read_modes) = \\\n",
    "            tf.dynamic_partition(self.interface_vec, partition, 10)\n",
    "        \n",
    "        #read vectors\n",
    "        read_keys = tf.reshape(read_keys,[self.num_heads, self.word_size]) #R*W\n",
    "        read_str = 1 + tf.nn.softplus(tf.expand_dims(read_str, 0)) #1*R\n",
    "        \n",
    "        #write vectors\n",
    "        write_key = tf.expand_dims(write_key, 0) #1*W\n",
    "        #help init our write weights\n",
    "        write_str = 1 + tf.nn.softplus(tf.expand_dims(write_str, 0)) #1*1\n",
    "        erase_vec = tf.nn.sigmoid(tf.expand_dims(erase_vec, 0)) #1*W\n",
    "        write_vec = tf.expand_dims(write_vec, 0) #1*W\n",
    "        \n",
    "        #the degree to which locations at read heads will be freed\n",
    "        free_gates = tf.nn.sigmoid(tf.expand_dims(free_gates, 0)) #1*R\n",
    "        #the fraction of writing that is being allocated in a new location\n",
    "        alloc_gate = tf.nn.sigmoid(alloc_gate) #1\n",
    "        #the amount of information to be written to memory\n",
    "        write_gate = tf.nn.sigmoid(write_gate) #1\n",
    "        #the softmax distribution between the three read modes (backward, forward, lookup)\n",
    "        #The read heads can use gates called read modes to switch between content lookup \n",
    "        #using a read key and reading out locations either forwards or backwards \n",
    "        #in the order they were written.\n",
    "        read_modes = tf.nn.softmax(tf.reshape(read_modes, [3, self.num_heads])) #3*R\n",
    "        \n",
    "        #used to calculate usage vector, what's available to write to?\n",
    "        retention_vec = tf.reduce_prod(1-free_gates*self.read_weights, reduction_indices=1)\n",
    "        #used to dynamically allocate memory\n",
    "        self.usage_vec = (self.usage_vec + self.write_weights - self.usage_vec * self.write_weights) * retention_vec\n",
    "\n",
    "        ##retreives the writing allocation weighting \n",
    "        alloc_weights = self.allocation_weighting() #N*1\n",
    "        #where to write to??\n",
    "        write_lookup_weights = self.content_lookup(write_key, write_str) #N*1\n",
    "        #define our write weights now that we know how much space to allocate for them and where to write to\n",
    "        self.write_weights = write_gate*(alloc_gate*alloc_weights + (1-alloc_gate)*write_lookup_weights)\n",
    "\n",
    "        #write erase, then write to memory!\n",
    "        self.mem_mat = self.mem_mat*(1-tf.matmul(self.write_weights, erase_vec)) + \\\n",
    "                       tf.matmul(self.write_weights, write_vec)\n",
    "\n",
    "        #As well as writing, the controller can read from multiple locations in memory. \n",
    "        #Memory can be searched based on the content of each location, or the associative \n",
    "        #temporal links can be followed forward and backward to recall information written \n",
    "        #in sequence or in reverse. (3rd attention mechanism)\n",
    "        \n",
    "        #updates and returns the temporal link matrix for the latest write\n",
    "        #given the precedence vector and the link matrix from previous step\n",
    "        nnweight_vec = tf.matmul(self.write_weights, tf.ones([1,self.num_words])) #N*N\n",
    "        self.link_mat = (1 - nnweight_vec - tf.transpose(nnweight_vec))*self.link_mat + \\\n",
    "                        tf.matmul(self.write_weights, self.precedence_weight, transpose_b=True)\n",
    "        self.link_mat *= tf.ones([self.num_words, self.num_words]) - tf.constant(np.identity(self.num_words, dtype=np.float32))\n",
    "\n",
    "        \n",
    "        self.precedence_weight = (1-tf.reduce_sum(self.write_weights, reduction_indices=0)) * \\\n",
    "                                 self.precedence_weight + self.write_weights\n",
    "        #3 modes - forward, backward, content lookup\n",
    "        forw_w = read_modes[2]*tf.matmul(self.link_mat, self.read_weights) #(N*N,N*R)->N*R\n",
    "        look_w = read_modes[1]*self.content_lookup(read_keys, read_str) #N*R\n",
    "        back_w = read_modes[0]*tf.matmul(self.link_mat, self.read_weights, transpose_a=True) #N*R\n",
    "\n",
    "        #use them to intiialize read weights\n",
    "        self.read_weights = back_w + look_w + forw_w #N*R\n",
    "        #create read vectors by applying read weights to memory matrix\n",
    "        self.read_vecs = tf.transpose(tf.matmul(self.mem_mat, self.read_weights, transpose_a=True)) #(W*N,N*R)^T->R*W\n",
    "\n",
    "        #multiply them together\n",
    "        read_vec_mut = tf.matmul(tf.reshape(self.read_vecs, [1, self.num_heads * self.word_size]),\n",
    "                                 self.read_vecs_out_weight)  # (1*RW, RW*Y)-> (1*Y)\n",
    "        \n",
    "        #return output + read vecs product\n",
    "        return self.nn_out+read_vec_mut\n",
    "\n",
    "    #output list of numbers (one hot encoded) by running the step function\n",
    "    def run(self):\n",
    "        big_out = []\n",
    "        for t, seq in enumerate(tf.unstack(self.i_data, axis=0)):\n",
    "            seq = tf.expand_dims(seq, 0)\n",
    "            y = self.step_m(seq)\n",
    "            big_out.append(y)\n",
    "        return tf.stack(big_out, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-4c932f79dd37>:35: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "iter: 0   loss: 0.697249174118 \n",
      "iter: 100   loss: 0.288413822651 \n",
      "iter: 200   loss: 0.217181891203 \n",
      "iter: 300   loss: 0.137070327997 \n",
      "iter: 400   loss: 0.100126214325 \n",
      "iter: 500   loss: 0.057627979666 \n",
      "iter: 600   loss: 0.0441081896424 \n",
      "iter: 700   loss: 0.0342821329832 \n",
      "iter: 800   loss: 0.0281143896282 \n",
      "iter: 900   loss: 0.0238682925701 \n",
      "iter: 1000   loss: 0.0208082795143 \n",
      "iter: 1100   loss: 0.0185230076313 \n",
      "iter: 1200   loss: 0.0167615525424 \n",
      "iter: 1300   loss: 0.0153656974435 \n",
      "iter: 1400   loss: 0.0142323896289 \n",
      "iter: 1500   loss: 0.0132922064513 \n",
      "iter: 1600   loss: 0.012497051619 \n",
      "iter: 1700   loss: 0.01181287691 \n",
      "iter: 1800   loss: 0.0129666253924 \n",
      "iter: 1900   loss: 0.249441966414 \n",
      "iter: 2000   loss: 0.225252330303 \n",
      "iter: 2100   loss: 0.20274142921 \n",
      "iter: 2200   loss: 0.174335032701 \n",
      "iter: 2300   loss: 0.148242369294 \n",
      "iter: 2400   loss: 0.124547980726 \n",
      "iter: 2500   loss: 0.10486727953 \n",
      "iter: 2600   loss: 0.0838196501136 \n",
      "iter: 2700   loss: 0.0657383799553 \n",
      "iter: 2800   loss: 0.0421641245484 \n",
      "iter: 2900   loss: 0.024766780436 \n",
      "iter: 3000   loss: 0.0214332137257 \n",
      "iter: 3100   loss: 0.0194343775511 \n",
      "iter: 3200   loss: 0.0179784633219 \n",
      "iter: 3300   loss: 0.0168283432722 \n",
      "iter: 3400   loss: 0.0158706493676 \n",
      "iter: 3500   loss: 0.0150419790298 \n",
      "iter: 3600   loss: 0.0143028926104 \n",
      "iter: 3700   loss: 0.0136289382353 \n",
      "iter: 3800   loss: 0.0130093032494 \n",
      "iter: 3900   loss: 0.0124427881092 \n",
      "iter: 4000   loss: 0.0119293099269 \n",
      "iter: 4100   loss: 0.0114661250263 \n",
      "iter: 4200   loss: 0.0110474890098 \n",
      "iter: 4300   loss: 0.0106655117124 \n",
      "iter: 4400   loss: 0.0103123486042 \n",
      "iter: 4500   loss: 0.00998188368976 \n",
      "iter: 4600   loss: 0.00966988317668 \n",
      "iter: 4700   loss: 0.00937351584435 \n",
      "iter: 4800   loss: 0.00909081846476 \n",
      "iter: 4900   loss: 0.00882037356496 \n",
      "iter: 5000   loss: 0.00856110267341 \n",
      "iter: 5100   loss: 0.00831213779747 \n",
      "iter: 5200   loss: 0.00807275529951 \n",
      "iter: 5300   loss: 0.00784232653677 \n",
      "iter: 5400   loss: 0.00762028992176 \n",
      "iter: 5500   loss: 0.00740613648668 \n",
      "iter: 5600   loss: 0.00719940382987 \n",
      "iter: 5700   loss: 0.00699966028333 \n",
      "iter: 5800   loss: 0.00680651236326 \n",
      "iter: 5900   loss: 0.00661960616708 \n",
      "iter: 6000   loss: 0.00643862551078 \n",
      "iter: 6100   loss: 0.00626329518855 \n",
      "iter: 6200   loss: 0.00609336933121 \n",
      "iter: 6300   loss: 0.00592863373458 \n",
      "iter: 6400   loss: 0.00576888211071 \n",
      "iter: 6500   loss: 0.00561391049996 \n",
      "iter: 6600   loss: 0.00546352285892 \n",
      "iter: 6700   loss: 0.00531752314419 \n",
      "iter: 6800   loss: 0.00517572415993 \n",
      "iter: 6900   loss: 0.00503795268014 \n",
      "iter: 7000   loss: 0.0049040406011 \n",
      "iter: 7100   loss: 0.00477384356782 \n",
      "iter: 7200   loss: 0.00464722327888 \n",
      "iter: 7300   loss: 0.00452406797558 \n",
      "iter: 7400   loss: 0.00440427381545 \n",
      "iter: 7500   loss: 0.00428776023909 \n",
      "iter: 7600   loss: 0.00417446531355 \n",
      "iter: 7700   loss: 0.00406433641911 \n",
      "iter: 7800   loss: 0.00395732745528 \n",
      "iter: 7900   loss: 0.00385339860804 \n",
      "iter: 8000   loss: 0.00375251215883 \n",
      "iter: 8100   loss: 0.00365464668721 \n",
      "iter: 8200   loss: 0.0035597840324 \n",
      "iter: 8300   loss: 0.00346790952608 \n",
      "iter: 8400   loss: 0.00337900337763 \n",
      "iter: 8500   loss: 0.0032930332236 \n",
      "iter: 8600   loss: 0.00320995459333 \n",
      "iter: 8700   loss: 0.00312970019877 \n",
      "iter: 8800   loss: 0.00305218948051 \n",
      "iter: 8900   loss: 0.00297732348554 \n",
      "iter: 9000   loss: 0.00290499161929 \n",
      "iter: 9100   loss: 0.00283507350832 \n",
      "iter: 9200   loss: 0.00276744482107 \n",
      "iter: 9300   loss: 0.00270198099315 \n",
      "iter: 9400   loss: 0.00824295729399 \n",
      "iter: 9500   loss: 0.00624416954815 \n",
      "iter: 9600   loss: 0.00536649115384 \n",
      "iter: 9700   loss: 0.00482245441526 \n",
      "iter: 9800   loss: 0.00444960780442 \n",
      "iter: 9900   loss: 0.00417758990079 \n",
      "iter: 10000   loss: 0.00396987237036 \n",
      "()\n",
      "final_i_data\n",
      "[[ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "final_o_data\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  1.  0.  0.]]\n",
      "predictions\n",
      "[[ -9.07644558  -9.72336769  -7.26317692  -9.54647541]\n",
      " [-15.18763733 -15.75201416  -8.62975216  -9.61322594]\n",
      " [ -9.66671848  -6.20337152  -7.75123215 -26.51697159]\n",
      " [-10.28835583 -11.68973351  -6.50088644  -5.19984961]\n",
      " [-15.41086388 -13.74402428  -8.41397858 -14.4151001 ]\n",
      " [-23.62148285 -22.05200577  -6.4646101   -9.28346348]\n",
      " [-18.27412033 -14.92560387   5.38609076 -10.68474388]\n",
      " [-27.02832031 -10.51839638  -9.04154968   5.69771194]\n",
      " [-11.17579079   4.97967625  -5.16888475  -4.5773983 ]\n",
      " [-25.25387764  -5.00152493 -12.15637016   4.97577953]\n",
      " [ -9.79953957  -5.40251207   6.1559391   -7.12371922]\n",
      " [-21.22710037   5.17974186 -10.91933346  -5.67307711]]\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def main(argv=None):\n",
    "\n",
    "    #generate the input output sequences, randomly intialized\n",
    "    num_seq = 10\n",
    "    seq_len = 6\n",
    "    seq_width = 4\n",
    "    iterations = 10000\n",
    "    con = np.random.randint(0, seq_width,size=seq_len)\n",
    "    seq = np.zeros((seq_len, seq_width))\n",
    "    seq[np.arange(seq_len), con] = 1\n",
    "    end = np.asarray([[-1]*seq_width])\n",
    "    zer = np.zeros((seq_len, seq_width))\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        #training time\n",
    "        with tf.Session() as sess:\n",
    "            #init the DNC\n",
    "            dnc = DNC(input_size=seq_width, output_size=seq_width, seq_len=seq_len, num_words=10, word_size=4, num_heads=1)\n",
    "            \n",
    "            #calculate the predicted output\n",
    "            output = tf.squeeze(dnc.run())\n",
    "            #compare prediction to reality, get loss via sigmoid cross entropy\n",
    "            loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output, labels=dnc.o_data))\n",
    "            #use regularizers for each layer of the controller\n",
    "            regularizers = (tf.nn.l2_loss(dnc.W1) + tf.nn.l2_loss(dnc.W2) +\n",
    "                            tf.nn.l2_loss(dnc.b1) + tf.nn.l2_loss(dnc.b2))\n",
    "            #to help the loss convergence faster\n",
    "            loss += 5e-4 * regularizers\n",
    "            #optimize the entire thing (memory + controller) using gradient descent. dope\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "            \n",
    "            #initialize input output pairs\n",
    "            tf.initialize_all_variables().run()\n",
    "            final_i_data = np.concatenate((seq, zer), axis=0)\n",
    "            final_o_data = np.concatenate((zer, seq), axis=0)\n",
    "            #for each iteration\n",
    "            for i in range(0, iterations+1):\n",
    "                #feed in each input output pair\n",
    "                feed_dict = {dnc.i_data: final_i_data, dnc.o_data: final_o_data}\n",
    "                #make predictions\n",
    "                l, _, predictions = sess.run([loss, optimizer, output], feed_dict=feed_dict)\n",
    "                if i%100==0:\n",
    "                    print(\"iter: {}   loss: {} \".format(i, l))\n",
    "            #print results\n",
    "            print()\n",
    "            print(\"final_i_data\")\n",
    "            print(final_i_data)\n",
    "            print(\"final_o_data\")\n",
    "            print(final_o_data)\n",
    "            print(\"predictions\")\n",
    "            print(predictions)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
